{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cb283ab1",
   "metadata": {},
   "source": [
    "ref: https://langfuse.com/guides/cookbook/example_pydantic_ai_mcp_agent_evaluation\n",
    "\n",
    "- trajactory(trace): Agentì˜ ì „ì²´ì ì¸ ì‹œí€€ìŠ¤\n",
    "- Agent ê°œë°œì—ì„œì˜ ë¬¸ì œì ì€\n",
    "    1. understanding: ì •í™•ížˆ ì–´ë–¤ íˆ´ì„ í˜¸ì¶œí•˜ê³  ì–´ë–¤ ì¶”ë¡ ì„ í•˜ëŠ” trajactoryì— ëŒ€í•´ ì‚¬ìš©ìžê°€ ëª¨ë¥´ëŠ” ê²½ìš°ê°€ ë§Žë‹¤.\n",
    "    2. specification: í”„ë¡¬í”„íŠ¸ë‚˜ few-shotì—ì„œ ì¢‹ì€ actionì— ëŒ€í•œ ë‚´ìš©ì´ ë¶„ëª…í•˜ê²Œ ë§í•˜ì§€ ì•Šì•„ì„œ agentê°€ ì˜ˆì¸¡í•˜ê¸° ì–´ë µê²Œ ëœë‹¤.\n",
    "    3. generalization: ì¼ë¶€ì˜ í‰ê°€ ë°ì´í„°ì—ì„œëŠ” ìž˜ ë™ìž‘í•˜ë”ë¼ë„ ì‹¤ì œ í™˜ê²½ì—ì„œ ì‹¤íŒ¨í•˜ëŠ” ê²½ìš°ê°€ ë§Žë‹¤.\n",
    "- í‰ê°€ 3ë‹¨ê³„\n",
    "    - Phase 1: Early Developoment (Manual Tracing)\n",
    "        - ê°œë°œ ë‹¨ê³„ì—ì„œ trajactoryë¥¼ ê³„ì† í™•ì¸í•˜ë©´ì„œ agent ì¶”ë¡  ê³¼ì •ì„ ë³´ë©° ì¸ì‚¬ì´íŠ¸ ì–»ëŠ”ë‹¤.\n",
    "    - Phase 2: First Users (Online Evaluation)\n",
    "        - ì‚¬ìš©ìž ë°˜ì‘ ë°ì´í„°ë¥¼ ì–»ê³ , ë¬¸ì œê°€ ìžˆì„ ê²ƒ ê°™ì€ trajactoryë¥¼ ë¦¬ë·°í•œë‹¤.\n",
    "    - Phase 3: Scaling (Offline Evaluation)\n",
    "        - ì„œë¹„ìŠ¤ ìŠ¤ì¼€ì¼ì´ ì»¤ì§€ë©´ì„œ ëª¨ë“  trajactoryë¥¼ ë¦¬ë·°í•˜ê¸° ì–´ë ¤ì›Œì§€ëŠ”ë°, í‘œì¤€ í‰ê°€ ë°ì´í„°ì…‹ì„ êµ¬ì¶•í•´ë‘ê³ , trajactory reviewë¥¼ ìžë™í™”ì‹œí‚¨ë‹¤.\n",
    "- LangfuseëŠ” phase 3ì˜ offline evaluationì— ì´ˆì ì´ ë§žì¶°ì ¸ìžˆë‹¤. í‰ê°€ ìžë™í™” ì „ëžµìœ¼ë¡œëŠ”\n",
    "    - Final Response: ì‚¬ìš©ìž inputê³¼ Agent Outputë§Œ ë³´ê³  í‰ê°€í•˜ëŠ” ì „ëžµìœ¼ë¡œ ê°„ë‹¨í•˜ë©´ì„œ ìœ ì—°í•˜ì§€ë§Œ, agentê°€ ì™œ failedí–ˆëŠ”ì§€ ì•Œ ìˆ˜ ì—†ë‹¤.\n",
    "    - Tranjactory: ë‚´ë¶€ ë™ìž‘ ì‹œí€€ìŠ¤ë¥¼ ê´€ì°°í•˜ë©° ì™œ ì‹¤íŒ¨í–ˆëŠ”ì§€ ì°¾ì„ ìˆ˜ ìžˆë„ë¡ ë•ëŠ”ë‹¤.\n",
    "    - Single Step: reasoning ë‹¨ê³„ê¹Œì§€ ëª¨ë‹ˆí„°ë§í•˜ë©° decision making ê³¼ì •ê¹Œì§€ ì‚´íŽ´ë³´ê³  ì™œ ì´ëŸ¬í•œ actionì´ ë‚˜ì™”ëŠ”ì§€ ì‚´íŽ´ë³¸ë‹¤."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "df870d4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "\n",
    "OPENAI_API_KEY = os.getenv(\"OPENAI_API_KEY\")\n",
    "LANGFUSE_PUBLIC_KEY = os.getenv(\"LANGFUSE_PUBLIC_KEY\")\n",
    "LANGFUSE_SECRET_KEY = os.getenv(\"LANGFUSE_SECRET_KEY\")\n",
    "LANGFUSE_HOST = os.getenv(\"LANGFUSE_HOST\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ae5727a2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Pydantic AI instrumentation enabled\n"
     ]
    }
   ],
   "source": [
    "from langfuse import get_client\n",
    "from pydantic_ai.agent import Agent\n",
    " \n",
    "langfuse = get_client()\n",
    "assert langfuse.auth_check(), \"Langfuse auth failed - check your keys\"\n",
    " \n",
    "Agent.instrument_all()\n",
    "print(\"âœ… Pydantic AI instrumentation enabled\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "245d479e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create agent\n",
    "\n",
    "from typing import Any\n",
    "from pydantic_ai import Agent, RunContext\n",
    "from pydantic_ai.mcp import MCPServerStreamableHTTP, CallToolFunc, ToolResult\n",
    " \n",
    "LANGFUSE_MCP_URL = \"https://langfuse.com/api/mcp\"\n",
    " \n",
    "async def run_agent(item, system_prompt=\"You are an expert on Langfuse. \", model=\"openai:gpt-4o-mini\"):\n",
    "    langfuse.update_current_trace(input=item.input)\n",
    " \n",
    "    tool_call_history = []\n",
    " \n",
    "    async def process_tool_call(\n",
    "        ctx: RunContext[Any],\n",
    "        call_tool: CallToolFunc,\n",
    "        tool_name: str,\n",
    "        args: dict[str, Any],\n",
    "    ) -> ToolResult:\n",
    "        tool_call_history.append({\"tool_name\": tool_name, \"args\": args})\n",
    "        return await call_tool(tool_name, args)\n",
    "    \n",
    "    langfuse_docs_server = MCPServerStreamableHTTP(\n",
    "        url=LANGFUSE_MCP_URL,\n",
    "        process_tool_call=process_tool_call,\n",
    "    )\n",
    " \n",
    "    agent = Agent(\n",
    "        model=model,\n",
    "        system_prompt=system_prompt,\n",
    "        toolsets=[langfuse_docs_server],\n",
    "    )\n",
    " \n",
    "    async with agent:\n",
    "        result = await agent.run(item.input[\"question\"])\n",
    "        \n",
    "        langfuse.update_current_trace(\n",
    "            output=result.output,\n",
    "            metadata={\"tool_call_history\": tool_call_history},\n",
    "        )\n",
    " \n",
    "        return result.output, tool_call_history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e87f108",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create evaluation dataset\n",
    "\n",
    "test_cases = [\n",
    "    {\n",
    "        \"input\": {\"question\": \"What is Langfuse?\"},\n",
    "        \"expected_output\": {\n",
    "            \"response_facts\": [\n",
    "                \"Open Source LLM Engineering Platform\",\n",
    "                \"Product modules: Tracing, Evaluation and Prompt Management\"\n",
    "            ],\n",
    "            \"trajectory\": [\"getLangfuseOverview\"],\n",
    "        }\n",
    "    },\n",
    "    {\n",
    "        \"input\": {\"question\": \"How to trace a python application with Langfuse?\"},\n",
    "        \"expected_output\": {\n",
    "            \"response_facts\": [\n",
    "                \"Python SDK, you can use the observe() decorator\",\n",
    "                \"Lots of integrations, LangChain, LlamaIndex, Pydantic AI, and many more.\"\n",
    "            ],\n",
    "            \"trajectory\": [\"getLangfuseOverview\", \"searchLangfuseDocs\"],\n",
    "            \"search_term\": \"Python Tracing\"\n",
    "        }\n",
    "    },\n",
    "    {\n",
    "        \"input\": {\"question\": \"How to connect to the Langfuse Docs MCP server?\"},\n",
    "        \"expected_output\": {\n",
    "            \"response_facts\": [\n",
    "                \"Connect via the MCP server endpoint: https://langfuse.com/api/mcp\",\n",
    "                \"Transport protocol: `streamableHttp`\"\n",
    "            ],\n",
    "            \"trajectory\": [\"getLangfuseOverview\"]\n",
    "        }\n",
    "    },\n",
    "    {\n",
    "        \"input\": {\"question\": \"How long are traces retained in langfuse?\"},\n",
    "        \"expected_output\": {\n",
    "            \"response_facts\": [\n",
    "                \"By default, traces are retained indefinitely\",\n",
    "                \"You can set custom data retention policy in the project settings\"\n",
    "            ],\n",
    "            \"trajectory\": [\"getLangfuseOverview\", \"searchLangfuseDocs\"],\n",
    "            \"search_term\": \"Data retention\"\n",
    "        }\n",
    "    }\n",
    "]\n",
    " \n",
    "DATASET_NAME = \"pydantic-ai-mcp-agent-evaluation\"\n",
    " \n",
    "dataset = langfuse.create_dataset(name=DATASET_NAME)\n",
    "for case in test_cases:\n",
    "    langfuse.create_dataset_item(\n",
    "        dataset_name=DATASET_NAME,\n",
    "        input=case[\"input\"],\n",
    "        expected_output=case[\"expected_output\"]\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "65ac0cb5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nYou are grading whether a student searched for the right information. The search term should correspond vaguely with the expected term.\\n\\n### Examples\\n\\nResponse: \"How can I contact support?\"\\nExpected search topics: Support\\nReasoning: Response searches for support.\\nScore: 1\\n\\nResponse: \"Deployment\"\\nExpected search topics: Tracing\\nReasoning: Response doesn\\'t match expected topic.\\nScore: 0\\n\\nResponse: (empty)\\nExpected search topics: (empty)\\nReasoning: No search expected, no search done.\\nScore: 1\\n\\n### New Student Response\\n\\nResponse: {{search}}\\nExpected search topics: {{expected_search_topic}}\\n'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# set evaluator in langfuse UI ('LLM-as-a-judge' > 'set up evaluator')\n",
    "\n",
    "#Final Response Evaluation (Black Box)\n",
    "\"\"\"\n",
    "You are a teacher grading a student based on the factual correctness of their statements.\n",
    " \n",
    "### Examples\n",
    " \n",
    "#### Example 1:\n",
    "- Response: \"The sun is shining brightly.\"\n",
    "- Facts to verify: [\"The sun is up.\", \"It is a beautiful day.\"]\n",
    "- Reasoning: The response includes both facts.\n",
    "- Score: 1\n",
    " \n",
    "#### Example 2:\n",
    "- Response: \"When I was in the kitchen, the dog was there\"\n",
    "- Facts to verify: [\"The cat is on the table.\", \"The dog is in the kitchen.\"]\n",
    "- Reasoning: The response mentions the dog but not the cat.\n",
    "- Score: 0\n",
    " \n",
    "### New Student Response\n",
    " \n",
    "- Response: {{response}}\n",
    "- Facts to verify: {{facts_to_verify}}\n",
    "\"\"\"\n",
    "\n",
    "## Tranjactory Evaluation (Glass Box)\n",
    "\"\"\"\n",
    "You are comparing two lists of strings. Check whether the lists contain exactly the same items. Order does not matter.\n",
    " \n",
    "## Examples\n",
    " \n",
    "Expected: [\"searchWeb\", \"visitWebsite\"]\n",
    "Output: [\"searchWeb\"]\n",
    "Reasoning: Output missing \"visitWebsite\".\n",
    "Score: 0\n",
    " \n",
    "Expected: [\"drawImage\", \"visitWebsite\", \"speak\"]\n",
    "Output: [\"visitWebsite\", \"speak\", \"drawImage\"]\n",
    "Reasoning: Output matches expected items.\n",
    "Score: 1\n",
    " \n",
    "Expected: [\"getNews\"]\n",
    "Output: [\"getNews\", \"watchTv\"]\n",
    "Reasoning: Output contains unexpected \"watchTv\".\n",
    "Score: 0\n",
    " \n",
    "## This Exercise\n",
    " \n",
    "Expected: {{expected}}\n",
    "Output: {{output}}\n",
    "\"\"\"\n",
    "\n",
    "## Search Quality Evaluation\n",
    "\"\"\"\n",
    "You are grading whether a student searched for the right information. The search term should correspond vaguely with the expected term.\n",
    " \n",
    "### Examples\n",
    " \n",
    "Response: \"How can I contact support?\"\n",
    "Expected search topics: Support\n",
    "Reasoning: Response searches for support.\n",
    "Score: 1\n",
    " \n",
    "Response: \"Deployment\"\n",
    "Expected search topics: Tracing\n",
    "Reasoning: Response doesn't match expected topic.\n",
    "Score: 0\n",
    " \n",
    "Response: (empty)\n",
    "Expected search topics: (empty)\n",
    "Reasoning: No search expected, no search done.\n",
    "Score: 1\n",
    " \n",
    "### New Student Response\n",
    " \n",
    "Response: {{search}}\n",
    "Expected search topics: {{expected_search_topic}}\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ed404b74",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Individual Results: Hidden (2 items)\\nðŸ’¡ Set include_item_results=True to view them\\n\\nâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\\nðŸ§ª Experiment: Production Model Test3\n",
      "ðŸ“‹ Run name: Production Model Test3 - 2025-12-04T06:13:22.101896Z - Monthly evaluation of our production model\\n2 items\\nðŸ”— Dataset Run:\\n   https://cloud.langfuse.com/project/cmiqs4ss0000xad07duzdlnpf/datasets/cmiqsg110001iad07w45c8vw9/runs/3a65e040-6636-4e53-a5ba-db17eeba2726\n"
     ]
    }
   ],
   "source": [
    "# run experiments\n",
    "\n",
    "dataset = langfuse.get_dataset(DATASET_NAME)\n",
    " \n",
    "result = dataset.run_experiment(\n",
    "    name=\"Production Model Test3\",\n",
    "    description=\"Monthly evaluation of our production model\",\n",
    "    task=run_agent\n",
    ")\n",
    " \n",
    "print(result.format())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "02e131b5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Individual Results: Hidden (1 items)\\nðŸ’¡ Set include_item_results=True to view them\\n\\nâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\\nðŸ§ª Experiment: Test: simple openai:gpt-5-mini\n",
      "ðŸ“‹ Run name: Test: simple openai:gpt-5-mini - 2025-12-04T03:28:25.883522Z - Comparing prompts and models\\n1 items\\nðŸ”— Dataset Run:\\n   https://cloud.langfuse.com/project/cmiqs4ss0000xad07duzdlnpf/datasets/cmiqsg110001iad07w45c8vw9/runs/f3314f0d-32c2-4e5a-96ae-8f8fe23f4d07\n",
      "Individual Results: Hidden (2 items)\\nðŸ’¡ Set include_item_results=True to view them\\n\\nâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\\nðŸ§ª Experiment: Test: simple openai:gpt-5-nano\n",
      "ðŸ“‹ Run name: Test: simple openai:gpt-5-nano - 2025-12-04T03:33:31.071440Z - Comparing prompts and models\\n2 items\\nðŸ”— Dataset Run:\\n   https://cloud.langfuse.com/project/cmiqs4ss0000xad07duzdlnpf/datasets/cmiqsg110001iad07w45c8vw9/runs/6e399d08-cee9-49f3-83f9-ab5e6c15a7cc\n",
      "Individual Results: Hidden (2 items)\\nðŸ’¡ Set include_item_results=True to view them\\n\\nâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\\nðŸ§ª Experiment: Test: nudge_search openai:gpt-5-mini\n",
      "ðŸ“‹ Run name: Test: nudge_search openai:gpt-5-mini - 2025-12-04T03:38:35.211398Z - Comparing prompts and models\\n2 items\\nðŸ”— Dataset Run:\\n   https://cloud.langfuse.com/project/cmiqs4ss0000xad07duzdlnpf/datasets/cmiqsg110001iad07w45c8vw9/runs/526c05d2-4413-4daa-87ee-2e6507ac63d1\n",
      "Individual Results: Hidden (3 items)\\nðŸ’¡ Set include_item_results=True to view them\\n\\nâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\\nðŸ§ª Experiment: Test: nudge_search openai:gpt-5-nano\n",
      "ðŸ“‹ Run name: Test: nudge_search openai:gpt-5-nano - 2025-12-04T03:43:40.687559Z - Comparing prompts and models\\n3 items\\nðŸ”— Dataset Run:\\n   https://cloud.langfuse.com/project/cmiqs4ss0000xad07duzdlnpf/datasets/cmiqsg110001iad07w45c8vw9/runs/7bb7356c-0dc8-4615-a2ec-15fe40ad0f0f\n"
     ]
    }
   ],
   "source": [
    "# compare multiple configurations\n",
    "\n",
    "from functools import partial\n",
    " \n",
    "system_prompts = {\n",
    "    \"simple\": (\n",
    "        \"You are an expert on Langfuse. \"\n",
    "        \"Answer user questions accurately and concisely using the available MCP tools. \"\n",
    "        \"Cite sources when appropriate.\"\n",
    "    ),\n",
    "    \"nudge_search\": (\n",
    "        \"You are an expert on Langfuse. \"\n",
    "        \"Answer user questions accurately and concisely using the available MCP tools. \"\n",
    "        \"Always cite sources when appropriate. \"\n",
    "        \"When unsure, use getLangfuseOverview then search the docs. You can use these tools multiple times.\"\n",
    "    )\n",
    "}\n",
    " \n",
    "models = [\"openai:gpt-5-mini\", \"openai:gpt-5-nano\"]\n",
    " \n",
    "dataset = langfuse.get_dataset(DATASET_NAME)\n",
    " \n",
    "for prompt_name, prompt_content in system_prompts.items():\n",
    "    for test_model in models:\n",
    "        task = partial(\n",
    "            run_agent,\n",
    "            system_prompt=prompt_content,\n",
    "            model=test_model,\n",
    "        )\n",
    " \n",
    "        result = dataset.run_experiment(\n",
    "            name=f\"Test: {prompt_name} {test_model}\",\n",
    "            description=\"Comparing prompts and models\",\n",
    "            task=task\n",
    "        )\n",
    " \n",
    "        print(result.format())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "368c2946",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
